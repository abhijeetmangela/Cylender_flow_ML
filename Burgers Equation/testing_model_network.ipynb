{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d9e9890590>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mu = torch.tensor(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forwardnett(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Forwardnett,self).__init__()\n",
    "        self.L1 = nn.Linear(102,100)\n",
    "        self.L2 = nn.Linear(100,100)\n",
    "        self.L3 = nn.Linear(100,100)\n",
    "        self.L4 = nn.Linear(100,100)\n",
    "        self.L5 = nn.Linear(100,100)\n",
    "        \n",
    "    def forward(self,y_0,x_loc_and_time):\n",
    "        input_final = torch.cat((y_0,x_loc_and_time),-1)\n",
    "\n",
    "        b = F.relu(self.L1(input_final))\n",
    "        b = F.relu(self.L2(b))\n",
    "        b = F.relu(self.L3(b))\n",
    "        b = F.relu(self.L4(b))\n",
    "        b = self.L5(b)\n",
    "\n",
    "        return b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Forwardnett().to(device)\n",
    "database = pd.read_csv('sin_pix.csv',index_col=0).dropna().to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self,transform=None):\n",
    "        self.initial_conditions = torch.from_numpy(database[:,0:100])#.requires_grad_(True)\n",
    "        self.x_location = torch.from_numpy(database[:,[100]])#.requires_grad_(True)\n",
    "        self.time_vale = torch.from_numpy(database[:,[101]])#.requires_grad_(True)\n",
    "        self.true_y_value = torch.from_numpy(database[:,[102]])#.requires_grad_(True)\n",
    "        self.n_samples = database.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.initial_conditions[index] , self.x_location[index] , self.time_vale[index] , self.true_y_value[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset_data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "train_size = int(0.7*dataset_data.__len__())\n",
    "test_size = dataset_data.__len__()-train_size\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "Burger_train_data , Burger_test_data = torch.utils.data.random_split(Data(),[train_size,test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=Burger_train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#data_iter = iter(train_loader)\n",
    "#data = data_iter.__next__()\n",
    "#Init_val , x_loc, time, y_value = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 200\n",
    "total_samples = len(train_loader)\n",
    "n_iterations = math.ceil(num_epoch*total_samples/(batch_size))\n",
    "print(total_samples,n_iterations)\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "\n",
    "loss_rec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\abhi\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5000, 1])) that is different to the input size (torch.Size([5000, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\dell\\anaconda3\\envs\\abhi\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([3100, 1])) that is different to the input size (torch.Size([3100, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] , Step [5/5] , Loss: 0.4848986864089966\n",
      "Epoch [2/200] , Step [5/5] , Loss: 0.4735386967658997\n",
      "Epoch [3/200] , Step [5/5] , Loss: 0.4783152341842651\n",
      "Epoch [4/200] , Step [5/5] , Loss: 0.4920364022254944\n",
      "Epoch [5/200] , Step [5/5] , Loss: 0.4937328100204468\n",
      "Epoch [6/200] , Step [5/5] , Loss: 0.4818539917469025\n",
      "Epoch [7/200] , Step [5/5] , Loss: 0.4879262149333954\n",
      "Epoch [8/200] , Step [5/5] , Loss: 0.4817936420440674\n",
      "Epoch [9/200] , Step [5/5] , Loss: 0.4917103946208954\n",
      "Epoch [10/200] , Step [5/5] , Loss: 0.4886447191238403\n",
      "Epoch [11/200] , Step [5/5] , Loss: 0.4923895299434662\n",
      "Epoch [12/200] , Step [5/5] , Loss: 0.4798378944396973\n",
      "Epoch [13/200] , Step [5/5] , Loss: 0.4846940934658051\n",
      "Epoch [14/200] , Step [5/5] , Loss: 0.4882165193557739\n",
      "Epoch [15/200] , Step [5/5] , Loss: 0.4777549505233765\n",
      "Epoch [16/200] , Step [5/5] , Loss: 0.4828372001647949\n",
      "Epoch [17/200] , Step [5/5] , Loss: 0.4876842796802521\n",
      "Epoch [18/200] , Step [5/5] , Loss: 0.4816552400588989\n",
      "Epoch [19/200] , Step [5/5] , Loss: 0.4683749973773956\n",
      "Epoch [20/200] , Step [5/5] , Loss: 0.4810828566551208\n",
      "Epoch [21/200] , Step [5/5] , Loss: 0.4846256077289581\n",
      "Epoch [22/200] , Step [5/5] , Loss: 0.4792826473712921\n",
      "Epoch [23/200] , Step [5/5] , Loss: 0.4793629944324493\n",
      "Epoch [24/200] , Step [5/5] , Loss: 0.4878368079662323\n",
      "Epoch [25/200] , Step [5/5] , Loss: 0.4820706546306610\n",
      "Epoch [26/200] , Step [5/5] , Loss: 0.4799073636531830\n",
      "Epoch [27/200] , Step [5/5] , Loss: 0.4853121936321259\n",
      "Epoch [28/200] , Step [5/5] , Loss: 0.4882541298866272\n",
      "Epoch [29/200] , Step [5/5] , Loss: 0.4785756468772888\n",
      "Epoch [30/200] , Step [5/5] , Loss: 0.4842310547828674\n",
      "Epoch [31/200] , Step [5/5] , Loss: 0.4900428354740143\n",
      "Epoch [32/200] , Step [5/5] , Loss: 0.4835075736045837\n",
      "Epoch [33/200] , Step [5/5] , Loss: 0.4862023293972015\n",
      "Epoch [34/200] , Step [5/5] , Loss: 0.4813536405563354\n",
      "Epoch [35/200] , Step [5/5] , Loss: 0.4864977896213531\n",
      "Epoch [36/200] , Step [5/5] , Loss: 0.4806583523750305\n",
      "Epoch [37/200] , Step [5/5] , Loss: 0.4743295311927795\n",
      "Epoch [38/200] , Step [5/5] , Loss: 0.4897015988826752\n",
      "Epoch [39/200] , Step [5/5] , Loss: 0.4915359914302826\n",
      "Epoch [40/200] , Step [5/5] , Loss: 0.4739034771919250\n",
      "Epoch [41/200] , Step [5/5] , Loss: 0.4838892221450806\n",
      "Epoch [42/200] , Step [5/5] , Loss: 0.4835465252399445\n",
      "Epoch [43/200] , Step [5/5] , Loss: 0.4868867993354797\n",
      "Epoch [44/200] , Step [5/5] , Loss: 0.4873237907886505\n",
      "Epoch [45/200] , Step [5/5] , Loss: 0.4856149256229401\n",
      "Epoch [46/200] , Step [5/5] , Loss: 0.4772030115127563\n",
      "Epoch [47/200] , Step [5/5] , Loss: 0.4760754108428955\n",
      "Epoch [48/200] , Step [5/5] , Loss: 0.4838169813156128\n",
      "Epoch [49/200] , Step [5/5] , Loss: 0.4793918728828430\n",
      "Epoch [50/200] , Step [5/5] , Loss: 0.4885554015636444\n",
      "Epoch [51/200] , Step [5/5] , Loss: 0.4753639996051788\n",
      "Epoch [52/200] , Step [5/5] , Loss: 0.4859776198863983\n",
      "Epoch [53/200] , Step [5/5] , Loss: 0.4721090793609619\n",
      "Epoch [54/200] , Step [5/5] , Loss: 0.4895982444286346\n",
      "Epoch [55/200] , Step [5/5] , Loss: 0.4826898276805878\n",
      "Epoch [56/200] , Step [5/5] , Loss: 0.4837097823619843\n",
      "Epoch [57/200] , Step [5/5] , Loss: 0.4835350215435028\n",
      "Epoch [58/200] , Step [5/5] , Loss: 0.4836300909519196\n",
      "Epoch [59/200] , Step [5/5] , Loss: 0.4810234010219574\n",
      "Epoch [60/200] , Step [5/5] , Loss: 0.4804878532886505\n",
      "Epoch [61/200] , Step [5/5] , Loss: 0.4937882423400879\n",
      "Epoch [62/200] , Step [5/5] , Loss: 0.4850835204124451\n",
      "Epoch [63/200] , Step [5/5] , Loss: 0.4795843362808228\n",
      "Epoch [64/200] , Step [5/5] , Loss: 0.4862655699253082\n",
      "Epoch [65/200] , Step [5/5] , Loss: 0.4838058948516846\n",
      "Epoch [66/200] , Step [5/5] , Loss: 0.4847464859485626\n",
      "Epoch [67/200] , Step [5/5] , Loss: 0.4882553815841675\n",
      "Epoch [68/200] , Step [5/5] , Loss: 0.4874064624309540\n",
      "Epoch [69/200] , Step [5/5] , Loss: 0.4779093861579895\n",
      "Epoch [70/200] , Step [5/5] , Loss: 0.4837642014026642\n",
      "Epoch [71/200] , Step [5/5] , Loss: 0.4889840185642242\n",
      "Epoch [72/200] , Step [5/5] , Loss: 0.4986503422260284\n",
      "Epoch [73/200] , Step [5/5] , Loss: 0.4787911772727966\n",
      "Epoch [74/200] , Step [5/5] , Loss: 0.4796621501445770\n",
      "Epoch [75/200] , Step [5/5] , Loss: 0.4782245457172394\n",
      "Epoch [76/200] , Step [5/5] , Loss: 0.4848070144653320\n",
      "Epoch [77/200] , Step [5/5] , Loss: 0.4913569986820221\n",
      "Epoch [78/200] , Step [5/5] , Loss: 0.4833895266056061\n",
      "Epoch [79/200] , Step [5/5] , Loss: 0.4867980778217316\n",
      "Epoch [80/200] , Step [5/5] , Loss: 0.4774815440177917\n",
      "Epoch [81/200] , Step [5/5] , Loss: 0.4870357811450958\n",
      "Epoch [82/200] , Step [5/5] , Loss: 0.4780053496360779\n",
      "Epoch [83/200] , Step [5/5] , Loss: 0.4786213636398315\n",
      "Epoch [84/200] , Step [5/5] , Loss: 0.4906792938709259\n",
      "Epoch [85/200] , Step [5/5] , Loss: 0.4936738908290863\n",
      "Epoch [86/200] , Step [5/5] , Loss: 0.4937095642089844\n",
      "Epoch [87/200] , Step [5/5] , Loss: 0.4859578609466553\n",
      "Epoch [88/200] , Step [5/5] , Loss: 0.4984006583690643\n",
      "Epoch [89/200] , Step [5/5] , Loss: 0.4736732840538025\n",
      "Epoch [90/200] , Step [5/5] , Loss: 0.4821973145008087\n",
      "Epoch [91/200] , Step [5/5] , Loss: 0.4788907170295715\n",
      "Epoch [92/200] , Step [5/5] , Loss: 0.4830555021762848\n",
      "Epoch [93/200] , Step [5/5] , Loss: 0.4806711077690125\n",
      "Epoch [94/200] , Step [5/5] , Loss: 0.4682293832302094\n",
      "Epoch [95/200] , Step [5/5] , Loss: 0.4818225800991058\n",
      "Epoch [96/200] , Step [5/5] , Loss: 0.4827141165733337\n",
      "Epoch [97/200] , Step [5/5] , Loss: 0.4811042249202728\n",
      "Epoch [98/200] , Step [5/5] , Loss: 0.4878200590610504\n",
      "Epoch [99/200] , Step [5/5] , Loss: 0.4871586561203003\n",
      "Epoch [100/200] , Step [5/5] , Loss: 0.4780246019363403\n",
      "Epoch [101/200] , Step [5/5] , Loss: 0.4817807972431183\n",
      "Epoch [102/200] , Step [5/5] , Loss: 0.4888778626918793\n",
      "Epoch [103/200] , Step [5/5] , Loss: 0.4855589866638184\n",
      "Epoch [104/200] , Step [5/5] , Loss: 0.4765318036079407\n",
      "Epoch [105/200] , Step [5/5] , Loss: 0.4755729436874390\n",
      "Epoch [106/200] , Step [5/5] , Loss: 0.4756239354610443\n",
      "Epoch [107/200] , Step [5/5] , Loss: 0.4828903675079346\n",
      "Epoch [108/200] , Step [5/5] , Loss: 0.4783797264099121\n",
      "Epoch [109/200] , Step [5/5] , Loss: 0.4885077476501465\n",
      "Epoch [110/200] , Step [5/5] , Loss: 0.4848474860191345\n",
      "Epoch [111/200] , Step [5/5] , Loss: 0.4814903140068054\n",
      "Epoch [112/200] , Step [5/5] , Loss: 0.4840537309646606\n",
      "Epoch [113/200] , Step [5/5] , Loss: 0.4846134185791016\n",
      "Epoch [114/200] , Step [5/5] , Loss: 0.4869754910469055\n",
      "Epoch [115/200] , Step [5/5] , Loss: 0.4833426475524902\n",
      "Epoch [116/200] , Step [5/5] , Loss: 0.4815202057361603\n",
      "Epoch [117/200] , Step [5/5] , Loss: 0.4684231877326965\n",
      "Epoch [118/200] , Step [5/5] , Loss: 0.4736929833889008\n",
      "Epoch [119/200] , Step [5/5] , Loss: 0.4847368896007538\n",
      "Epoch [120/200] , Step [5/5] , Loss: 0.4741280674934387\n",
      "Epoch [121/200] , Step [5/5] , Loss: 0.4835180044174194\n",
      "Epoch [122/200] , Step [5/5] , Loss: 0.4884597659111023\n",
      "Epoch [123/200] , Step [5/5] , Loss: 0.4797959029674530\n",
      "Epoch [124/200] , Step [5/5] , Loss: 0.4860298931598663\n",
      "Epoch [125/200] , Step [5/5] , Loss: 0.4835748076438904\n",
      "Epoch [126/200] , Step [5/5] , Loss: 0.4811157286167145\n",
      "Epoch [127/200] , Step [5/5] , Loss: 0.4854087233543396\n",
      "Epoch [128/200] , Step [5/5] , Loss: 0.4815702736377716\n",
      "Epoch [129/200] , Step [5/5] , Loss: 0.4849670827388763\n",
      "Epoch [130/200] , Step [5/5] , Loss: 0.4863954782485962\n",
      "Epoch [131/200] , Step [5/5] , Loss: 0.4834914803504944\n",
      "Epoch [132/200] , Step [5/5] , Loss: 0.4945652186870575\n",
      "Epoch [133/200] , Step [5/5] , Loss: 0.4917449653148651\n",
      "Epoch [134/200] , Step [5/5] , Loss: 0.4877220690250397\n",
      "Epoch [135/200] , Step [5/5] , Loss: 0.4844741821289062\n",
      "Epoch [136/200] , Step [5/5] , Loss: 0.4899339079856873\n",
      "Epoch [137/200] , Step [5/5] , Loss: 0.4814309477806091\n",
      "Epoch [138/200] , Step [5/5] , Loss: 0.4838417470455170\n",
      "Epoch [139/200] , Step [5/5] , Loss: 0.4820643067359924\n",
      "Epoch [140/200] , Step [5/5] , Loss: 0.4823427796363831\n",
      "Epoch [141/200] , Step [5/5] , Loss: 0.4863547980785370\n",
      "Epoch [142/200] , Step [5/5] , Loss: 0.4873591363430023\n",
      "Epoch [143/200] , Step [5/5] , Loss: 0.4788900315761566\n",
      "Epoch [144/200] , Step [5/5] , Loss: 0.4833516180515289\n",
      "Epoch [145/200] , Step [5/5] , Loss: 0.4811638593673706\n",
      "Epoch [146/200] , Step [5/5] , Loss: 0.4820601344108582\n",
      "Epoch [147/200] , Step [5/5] , Loss: 0.4805434048175812\n",
      "Epoch [148/200] , Step [5/5] , Loss: 0.4834582805633545\n",
      "Epoch [149/200] , Step [5/5] , Loss: 0.4857232272624969\n",
      "Epoch [150/200] , Step [5/5] , Loss: 0.4870602786540985\n",
      "Epoch [151/200] , Step [5/5] , Loss: 0.4753464162349701\n",
      "Epoch [152/200] , Step [5/5] , Loss: 0.4806150197982788\n",
      "Epoch [153/200] , Step [5/5] , Loss: 0.4947524070739746\n",
      "Epoch [154/200] , Step [5/5] , Loss: 0.4831592738628387\n",
      "Epoch [155/200] , Step [5/5] , Loss: 0.4916457235813141\n",
      "Epoch [156/200] , Step [5/5] , Loss: 0.4807812869548798\n",
      "Epoch [157/200] , Step [5/5] , Loss: 0.4834359288215637\n",
      "Epoch [158/200] , Step [5/5] , Loss: 0.4844999909400940\n",
      "Epoch [159/200] , Step [5/5] , Loss: 0.4799075722694397\n",
      "Epoch [160/200] , Step [5/5] , Loss: 0.4809066951274872\n",
      "Epoch [161/200] , Step [5/5] , Loss: 0.4940732717514038\n",
      "Epoch [162/200] , Step [5/5] , Loss: 0.4828890264034271\n",
      "Epoch [163/200] , Step [5/5] , Loss: 0.4827445149421692\n",
      "Epoch [164/200] , Step [5/5] , Loss: 0.4743838608264923\n",
      "Epoch [165/200] , Step [5/5] , Loss: 0.4837555587291718\n",
      "Epoch [166/200] , Step [5/5] , Loss: 0.4868789315223694\n",
      "Epoch [167/200] , Step [5/5] , Loss: 0.4796095788478851\n",
      "Epoch [168/200] , Step [5/5] , Loss: 0.4885108470916748\n",
      "Epoch [169/200] , Step [5/5] , Loss: 0.4889323711395264\n",
      "Epoch [170/200] , Step [5/5] , Loss: 0.4906322062015533\n",
      "Epoch [171/200] , Step [5/5] , Loss: 0.4750917851924896\n",
      "Epoch [172/200] , Step [5/5] , Loss: 0.4849352836608887\n",
      "Epoch [173/200] , Step [5/5] , Loss: 0.4872648715972900\n",
      "Epoch [174/200] , Step [5/5] , Loss: 0.4715915918350220\n",
      "Epoch [175/200] , Step [5/5] , Loss: 0.4840055406093597\n",
      "Epoch [176/200] , Step [5/5] , Loss: 0.4800270199775696\n",
      "Epoch [177/200] , Step [5/5] , Loss: 0.4783650636672974\n",
      "Epoch [178/200] , Step [5/5] , Loss: 0.4758499860763550\n",
      "Epoch [179/200] , Step [5/5] , Loss: 0.4867941141128540\n",
      "Epoch [180/200] , Step [5/5] , Loss: 0.4910962581634521\n",
      "Epoch [181/200] , Step [5/5] , Loss: 0.4918623864650726\n",
      "Epoch [182/200] , Step [5/5] , Loss: 0.4767812490463257\n",
      "Epoch [183/200] , Step [5/5] , Loss: 0.4869661331176758\n",
      "Epoch [184/200] , Step [5/5] , Loss: 0.4825892150402069\n",
      "Epoch [185/200] , Step [5/5] , Loss: 0.4788043797016144\n",
      "Epoch [186/200] , Step [5/5] , Loss: 0.4907831549644470\n",
      "Epoch [187/200] , Step [5/5] , Loss: 0.4863154292106628\n",
      "Epoch [188/200] , Step [5/5] , Loss: 0.4831247925758362\n",
      "Epoch [189/200] , Step [5/5] , Loss: 0.4881965816020966\n",
      "Epoch [190/200] , Step [5/5] , Loss: 0.4869330525398254\n",
      "Epoch [191/200] , Step [5/5] , Loss: 0.4955004155635834\n",
      "Epoch [192/200] , Step [5/5] , Loss: 0.4685959815979004\n",
      "Epoch [193/200] , Step [5/5] , Loss: 0.4960891008377075\n",
      "Epoch [194/200] , Step [5/5] , Loss: 0.4888211190700531\n",
      "Epoch [195/200] , Step [5/5] , Loss: 0.4770941436290741\n",
      "Epoch [196/200] , Step [5/5] , Loss: 0.4698041379451752\n",
      "Epoch [197/200] , Step [5/5] , Loss: 0.4735793769359589\n",
      "Epoch [198/200] , Step [5/5] , Loss: 0.4769760072231293\n",
      "Epoch [199/200] , Step [5/5] , Loss: 0.4880718886852264\n",
      "Epoch [200/200] , Step [5/5] , Loss: 0.4946946203708649\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i , (input_init_conditions,input_x_loc,input_time,Actual_y) in enumerate(train_loader):\n",
    "        input1 = input_init_conditions\n",
    "        input1 = input1.to(device)\n",
    "\n",
    "        input2 = torch.cat((input_x_loc,input_time),-1)\n",
    "        input2 = input2.to(device)\n",
    "\n",
    "        Actual_y = Actual_y.to(device)\n",
    "\n",
    "        Outputs = model(input1,input2)\n",
    "\n",
    "        #input2_BC1 = torch.cat((torch.zeros(input_time.size(0),1),input_time),-1).to(device)\n",
    "        #target_BC1 = torch.zeros(input_time.size(0))\n",
    "        #predicted_BC1 = model(input1,input2_BC1)\n",
    "        #loss_BC1 = torch.mean((predicted_BC1-target_BC1)**2)\n",
    "\n",
    "        #input2_BC2 = torch.cat((torch.ones(input_time.size(0),1),input_time),-1).to(device)\n",
    "        #target_BC2 = torch.zeros(input_time.size(0))\n",
    "        #predicted_BC2 = model(input1,input2_BC2)\n",
    "        #loss_BC2 = torch.mean((predicted_BC2-target_BC2)**2)\n",
    "\n",
    "        #Physics_loss = \n",
    "\n",
    "        #Physics_loss = torch.mean()\n",
    "        \n",
    "        loss = criterion(Outputs,Actual_y) # + 1000*(loss_BC1 + loss_BC2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()    \n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        loss_rec.append(loss.item())\n",
    "\n",
    "        if (i+1) % 5 ==0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epoch}] , Step [{i+1}/{total_samples}] , Loss: {loss.item():.16f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abhi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
